{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "BERT_huggingface_IMDB Movie review.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "omfN5AZ9ys_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8aea8a9-0813-477a-8eb4-7f49a31aeafb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrvPcIuR1Qnt"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "imdb = \"/content/gdrive/My Drive/IMDB Dataset_bert_210219.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYhj-RIeybIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3135e622-5e6e-46e4-90d1-bfb1b69549c7"
      },
      "source": [
        "import pickle as pc\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "\n",
        "print(\"Pytorch Version: \", torch.__version__)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch Version:  1.8.0+cu101\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrMn70gSV3Ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0204378-0b79-4e96-9df3-9941c8c98f9a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1In5hQtPybIs"
      },
      "source": [
        "def load_data(filename):\n",
        "    data = list()\n",
        "    label = list()\n",
        "    \n",
        "    f = open(filename, 'r', encoding='latin1')\n",
        "    reader = csv.reader(f)\n",
        "    for idx, line in enumerate(reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "\n",
        "        data.append(line[0])\n",
        "        label.append(int(line[2]))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    assert len(data) == len(label)\n",
        "    return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBIeV1TybIv"
      },
      "source": [
        "imdb_data, imdb_label = load_data(imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjZNhURn4il1",
        "outputId": "3d654d43-fdc3-4ce9-f5f9-1b1dd55bfcae"
      },
      "source": [
        "print(\"Size of imdb data: {}\".format(len(imdb_data)))\n",
        "print(\"Size of imdb label: {}\".format(len(imdb_label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of imdb data: 50000\n",
            "Size of imdb label: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL-P6sSM4uFX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data, train_label, test_label = train_test_split(imdb_data, imdb_label, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43O3FuU7ybI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d710d57f-a510-43de-ce6f-f42e1a8cccef"
      },
      "source": [
        "print(\"Size of train data: {}\".format(len(train_data)))\n",
        "print(\"Size of train label: {}\".format(len(train_label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train data: 40000\n",
            "Size of train label: 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uq8AHTb48_r"
      },
      "source": [
        "import numpy as np\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kks51eZ9ybJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9265710-8b9a-43a9-89f9-5b1258f56167"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print(\"Loading BERT tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTwSHYz1ybJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acde120c-f885-4bdf-e340-dbf876d5be12"
      },
      "source": [
        "print(\"Original: \", train_data[0])\n",
        "print()\n",
        "\n",
        "print(\"Tokenized: \", tokenizer.tokenize(train_data[0]))\n",
        "print()\n",
        "\n",
        "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0])))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  That's what I kept asking myself during the many fights, screaming matches, swearing and general mayhem that permeate the 84 minutes. The comparisons also stand up when you think of the one-dimensional characters, who have so little depth that it is virtually impossible to care what happens to them. They are just badly written cyphers for the director to hang his multicultural beliefs on, a topic that has been done much better in other dramas both on TV and the cinema.<br /><br />I must confess, I'm not really one for spotting bad performances during a film, but it must be said that Nichola Burley (as the heroine's slutty best friend) and Wasim Zakir (as the nasty, bullying brother) were absolutely terrible. I don't know what acting school they graduated from, but if I was them I'd apply for a full refund post haste. Only Samina Awan in the lead role manages to impress in a cast of so-called British talent that we'll probably never hear from again. At least, that's the hope. Next time, hire a different scout.<br /><br />Another intriguing thought is the hideously fashionable soundtrack featuring the likes of Snow Patrol, Ian Brown and Keane. Now, I'm a bit of a music fan and I'm familiar with most of these artists output, but I didn't recognise any of the tracks during this movie (apart from the omnipresent \"Run\"). B-sides, anyone? We get many, many musical montages which telegraph how we're suppose to feel. These are accompanied by such startlingly original images as couples kissing by a swollen lake and canoodling in doorways. This is a problem, as none of the songs convey the mood efficiently, and we realise the director lacks the ability to carry the emotional journey to the audience through storytelling and dialogue alone.<br /><br />The ending is presumably meant to be just desserts, as everybody gets their comeuppance and there is at least one big shock in store.. But I remained resolutely unmoved because the script had given me no-one to root for. It's not enough to tackle a hot-button issue, you have to actually give us a plot that hasn't already been done to death and individuals who are more than window dressing. As it stands, this film is a noble failure, with only the promising lead actress and a few mildly diverting punch-ups to save it from the bin. 4/10. Must try harder..\n",
            "\n",
            "Tokenized:  ['that', \"'\", 's', 'what', 'i', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fights', ',', 'screaming', 'matches', ',', 'swearing', 'and', 'general', 'mayhem', 'that', 'per', '##me', '##ate', 'the', '84', 'minutes', '.', 'the', 'comparisons', 'also', 'stand', 'up', 'when', 'you', 'think', 'of', 'the', 'one', '-', 'dimensional', 'characters', ',', 'who', 'have', 'so', 'little', 'depth', 'that', 'it', 'is', 'virtually', 'impossible', 'to', 'care', 'what', 'happens', 'to', 'them', '.', 'they', 'are', 'just', 'badly', 'written', 'cy', '##pher', '##s', 'for', 'the', 'director', 'to', 'hang', 'his', 'multicultural', 'beliefs', 'on', ',', 'a', 'topic', 'that', 'has', 'been', 'done', 'much', 'better', 'in', 'other', 'dramas', 'both', 'on', 'tv', 'and', 'the', 'cinema', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'must', 'confess', ',', 'i', \"'\", 'm', 'not', 'really', 'one', 'for', 'spotting', 'bad', 'performances', 'during', 'a', 'film', ',', 'but', 'it', 'must', 'be', 'said', 'that', 'nic', '##hol', '##a', 'bu', '##rley', '(', 'as', 'the', 'heroine', \"'\", 's', 'sl', '##ut', '##ty', 'best', 'friend', ')', 'and', 'was', '##im', 'za', '##kir', '(', 'as', 'the', 'nasty', ',', 'bullying', 'brother', ')', 'were', 'absolutely', 'terrible', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'acting', 'school', 'they', 'graduated', 'from', ',', 'but', 'if', 'i', 'was', 'them', 'i', \"'\", 'd', 'apply', 'for', 'a', 'full', 'ref', '##und', 'post', 'haste', '.', 'only', 'sami', '##na', 'aw', '##an', 'in', 'the', 'lead', 'role', 'manages', 'to', 'impress', 'in', 'a', 'cast', 'of', 'so', '-', 'called', 'british', 'talent', 'that', 'we', \"'\", 'll', 'probably', 'never', 'hear', 'from', 'again', '.', 'at', 'least', ',', 'that', \"'\", 's', 'the', 'hope', '.', 'next', 'time', ',', 'hire', 'a', 'different', 'scout', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'another', 'intriguing', 'thought', 'is', 'the', 'hideous', '##ly', 'fashionable', 'soundtrack', 'featuring', 'the', 'likes', 'of', 'snow', 'patrol', ',', 'ian', 'brown', 'and', 'keane', '.', 'now', ',', 'i', \"'\", 'm', 'a', 'bit', 'of', 'a', 'music', 'fan', 'and', 'i', \"'\", 'm', 'familiar', 'with', 'most', 'of', 'these', 'artists', 'output', ',', 'but', 'i', 'didn', \"'\", 't', 'recognise', 'any', 'of', 'the', 'tracks', 'during', 'this', 'movie', '(', 'apart', 'from', 'the', 'om', '##ni', '##pres', '##ent', '\"', 'run', '\"', ')', '.', 'b', '-', 'sides', ',', 'anyone', '?', 'we', 'get', 'many', ',', 'many', 'musical', 'mont', '##ages', 'which', 'telegraph', 'how', 'we', \"'\", 're', 'suppose', 'to', 'feel', '.', 'these', 'are', 'accompanied', 'by', 'such', 'startling', '##ly', 'original', 'images', 'as', 'couples', 'kissing', 'by', 'a', 'swollen', 'lake', 'and', 'can', '##ood', '##ling', 'in', 'doorway', '##s', '.', 'this', 'is', 'a', 'problem', ',', 'as', 'none', 'of', 'the', 'songs', 'convey', 'the', 'mood', 'efficiently', ',', 'and', 'we', 'realise', 'the', 'director', 'lacks', 'the', 'ability', 'to', 'carry', 'the', 'emotional', 'journey', 'to', 'the', 'audience', 'through', 'storytelling', 'and', 'dialogue', 'alone', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'ending', 'is', 'presumably', 'meant', 'to', 'be', 'just', 'dessert', '##s', ',', 'as', 'everybody', 'gets', 'their', 'come', '##up', '##pan', '##ce', 'and', 'there', 'is', 'at', 'least', 'one', 'big', 'shock', 'in', 'store', '.', '.', 'but', 'i', 'remained', 'res', '##ol', '##ute', '##ly', 'un', '##mo', '##ved', 'because', 'the', 'script', 'had', 'given', 'me', 'no', '-', 'one', 'to', 'root', 'for', '.', 'it', \"'\", 's', 'not', 'enough', 'to', 'tackle', 'a', 'hot', '-', 'button', 'issue', ',', 'you', 'have', 'to', 'actually', 'give', 'us', 'a', 'plot', 'that', 'hasn', \"'\", 't', 'already', 'been', 'done', 'to', 'death', 'and', 'individuals', 'who', 'are', 'more', 'than', 'window', 'dressing', '.', 'as', 'it', 'stands', ',', 'this', 'film', 'is', 'a', 'noble', 'failure', ',', 'with', 'only', 'the', 'promising', 'lead', 'actress', 'and', 'a', 'few', 'mildly', 'divert', '##ing', 'punch', '-', 'ups', 'to', 'save', 'it', 'from', 'the', 'bin', '.', '4', '/', '10', '.', 'must', 'try', 'harder', '.', '.']\n",
            "\n",
            "Token IDs:  [2008, 1005, 1055, 2054, 1045, 2921, 4851, 2870, 2076, 1996, 2116, 9590, 1010, 7491, 3503, 1010, 25082, 1998, 2236, 26865, 2008, 2566, 4168, 3686, 1996, 6391, 2781, 1012, 1996, 18539, 2036, 3233, 2039, 2043, 2017, 2228, 1997, 1996, 2028, 1011, 8789, 3494, 1010, 2040, 2031, 2061, 2210, 5995, 2008, 2009, 2003, 8990, 5263, 2000, 2729, 2054, 6433, 2000, 2068, 1012, 2027, 2024, 2074, 6649, 2517, 22330, 27921, 2015, 2005, 1996, 2472, 2000, 6865, 2010, 27135, 9029, 2006, 1010, 1037, 8476, 2008, 2038, 2042, 2589, 2172, 2488, 1999, 2060, 16547, 2119, 2006, 2694, 1998, 1996, 5988, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2442, 18766, 1010, 1045, 1005, 1049, 2025, 2428, 2028, 2005, 27963, 2919, 4616, 2076, 1037, 2143, 1010, 2021, 2009, 2442, 2022, 2056, 2008, 27969, 14854, 2050, 20934, 12866, 1006, 2004, 1996, 18869, 1005, 1055, 22889, 4904, 3723, 2190, 2767, 1007, 1998, 2001, 5714, 23564, 23630, 1006, 2004, 1996, 11808, 1010, 18917, 2567, 1007, 2020, 7078, 6659, 1012, 1045, 2123, 1005, 1056, 2113, 2054, 3772, 2082, 2027, 3852, 2013, 1010, 2021, 2065, 1045, 2001, 2068, 1045, 1005, 1040, 6611, 2005, 1037, 2440, 25416, 8630, 2695, 24748, 1012, 2069, 17015, 2532, 22091, 2319, 1999, 1996, 2599, 2535, 9020, 2000, 17894, 1999, 1037, 3459, 1997, 2061, 1011, 2170, 2329, 5848, 2008, 2057, 1005, 2222, 2763, 2196, 2963, 2013, 2153, 1012, 2012, 2560, 1010, 2008, 1005, 1055, 1996, 3246, 1012, 2279, 2051, 1010, 10887, 1037, 2367, 7464, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2178, 23824, 2245, 2003, 1996, 22293, 2135, 19964, 6050, 3794, 1996, 7777, 1997, 4586, 6477, 1010, 4775, 2829, 1998, 27228, 1012, 2085, 1010, 1045, 1005, 1049, 1037, 2978, 1997, 1037, 2189, 5470, 1998, 1045, 1005, 1049, 5220, 2007, 2087, 1997, 2122, 3324, 6434, 1010, 2021, 1045, 2134, 1005, 1056, 17614, 2151, 1997, 1996, 3162, 2076, 2023, 3185, 1006, 4237, 2013, 1996, 18168, 3490, 28994, 4765, 1000, 2448, 1000, 1007, 1012, 1038, 1011, 3903, 1010, 3087, 1029, 2057, 2131, 2116, 1010, 2116, 3315, 18318, 13923, 2029, 10013, 2129, 2057, 1005, 2128, 6814, 2000, 2514, 1012, 2122, 2024, 5642, 2011, 2107, 19828, 2135, 2434, 4871, 2004, 6062, 7618, 2011, 1037, 13408, 2697, 1998, 2064, 17139, 2989, 1999, 7086, 2015, 1012, 2023, 2003, 1037, 3291, 1010, 2004, 3904, 1997, 1996, 2774, 16636, 1996, 6888, 18228, 1010, 1998, 2057, 19148, 1996, 2472, 14087, 1996, 3754, 2000, 4287, 1996, 6832, 4990, 2000, 1996, 4378, 2083, 20957, 1998, 7982, 2894, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 4566, 2003, 10712, 3214, 2000, 2022, 2074, 18064, 2015, 1010, 2004, 7955, 4152, 2037, 2272, 6279, 9739, 3401, 1998, 2045, 2003, 2012, 2560, 2028, 2502, 5213, 1999, 3573, 1012, 1012, 2021, 1045, 2815, 24501, 4747, 10421, 2135, 4895, 5302, 7178, 2138, 1996, 5896, 2018, 2445, 2033, 2053, 1011, 2028, 2000, 7117, 2005, 1012, 2009, 1005, 1055, 2025, 2438, 2000, 11147, 1037, 2980, 1011, 6462, 3277, 1010, 2017, 2031, 2000, 2941, 2507, 2149, 1037, 5436, 2008, 8440, 1005, 1056, 2525, 2042, 2589, 2000, 2331, 1998, 3633, 2040, 2024, 2062, 2084, 3332, 11225, 1012, 2004, 2009, 4832, 1010, 2023, 2143, 2003, 1037, 7015, 4945, 1010, 2007, 2069, 1996, 10015, 2599, 3883, 1998, 1037, 2261, 19499, 27345, 2075, 8595, 1011, 11139, 2000, 3828, 2009, 2013, 1996, 8026, 1012, 1018, 1013, 2184, 1012, 2442, 3046, 6211, 1012, 1012]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArPy88KybJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71063dc7-9972-48f2-f9a6-bc0a7d313ab0"
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for sent in train_data:\n",
        "    \n",
        "    encoded_sent = tokenizer.encode(sent, \n",
        "                                    add_special_tokens=True,\n",
        "                                    max_length = 256)\n",
        " \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "print(\"Original: \", train_data[0])\n",
        "print()\n",
        "print(\"Token IDs: \", input_ids[0])\n",
        "\n",
        "print(\"\\n[CLS] token: {:}, ID: {:}\".format(tokenizer.cls_token, tokenizer.cls_token_id))\n",
        "print(\"\\n[PAD] token: {:}, ID: {:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "print(\"\\n[SEP] token: {:}, ID: {:}\".format(tokenizer.sep_token, tokenizer.sep_token_id))\n",
        "print(\"\\nTokenized: \", tokenizer.convert_ids_to_tokens(input_ids[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  That's what I kept asking myself during the many fights, screaming matches, swearing and general mayhem that permeate the 84 minutes. The comparisons also stand up when you think of the one-dimensional characters, who have so little depth that it is virtually impossible to care what happens to them. They are just badly written cyphers for the director to hang his multicultural beliefs on, a topic that has been done much better in other dramas both on TV and the cinema.<br /><br />I must confess, I'm not really one for spotting bad performances during a film, but it must be said that Nichola Burley (as the heroine's slutty best friend) and Wasim Zakir (as the nasty, bullying brother) were absolutely terrible. I don't know what acting school they graduated from, but if I was them I'd apply for a full refund post haste. Only Samina Awan in the lead role manages to impress in a cast of so-called British talent that we'll probably never hear from again. At least, that's the hope. Next time, hire a different scout.<br /><br />Another intriguing thought is the hideously fashionable soundtrack featuring the likes of Snow Patrol, Ian Brown and Keane. Now, I'm a bit of a music fan and I'm familiar with most of these artists output, but I didn't recognise any of the tracks during this movie (apart from the omnipresent \"Run\"). B-sides, anyone? We get many, many musical montages which telegraph how we're suppose to feel. These are accompanied by such startlingly original images as couples kissing by a swollen lake and canoodling in doorways. This is a problem, as none of the songs convey the mood efficiently, and we realise the director lacks the ability to carry the emotional journey to the audience through storytelling and dialogue alone.<br /><br />The ending is presumably meant to be just desserts, as everybody gets their comeuppance and there is at least one big shock in store.. But I remained resolutely unmoved because the script had given me no-one to root for. It's not enough to tackle a hot-button issue, you have to actually give us a plot that hasn't already been done to death and individuals who are more than window dressing. As it stands, this film is a noble failure, with only the promising lead actress and a few mildly diverting punch-ups to save it from the bin. 4/10. Must try harder..\n",
            "\n",
            "Token IDs:  [101, 2008, 1005, 1055, 2054, 1045, 2921, 4851, 2870, 2076, 1996, 2116, 9590, 1010, 7491, 3503, 1010, 25082, 1998, 2236, 26865, 2008, 2566, 4168, 3686, 1996, 6391, 2781, 1012, 1996, 18539, 2036, 3233, 2039, 2043, 2017, 2228, 1997, 1996, 2028, 1011, 8789, 3494, 1010, 2040, 2031, 2061, 2210, 5995, 2008, 2009, 2003, 8990, 5263, 2000, 2729, 2054, 6433, 2000, 2068, 1012, 2027, 2024, 2074, 6649, 2517, 22330, 27921, 2015, 2005, 1996, 2472, 2000, 6865, 2010, 27135, 9029, 2006, 1010, 1037, 8476, 2008, 2038, 2042, 2589, 2172, 2488, 1999, 2060, 16547, 2119, 2006, 2694, 1998, 1996, 5988, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2442, 18766, 1010, 1045, 1005, 1049, 2025, 2428, 2028, 2005, 27963, 2919, 4616, 2076, 1037, 2143, 1010, 2021, 2009, 2442, 2022, 2056, 2008, 27969, 14854, 2050, 20934, 12866, 1006, 2004, 1996, 18869, 1005, 1055, 22889, 4904, 3723, 2190, 2767, 1007, 1998, 2001, 5714, 23564, 23630, 1006, 2004, 1996, 11808, 1010, 18917, 2567, 1007, 2020, 7078, 6659, 1012, 1045, 2123, 1005, 1056, 2113, 2054, 3772, 2082, 2027, 3852, 2013, 1010, 2021, 2065, 1045, 2001, 2068, 1045, 1005, 1040, 6611, 2005, 1037, 2440, 25416, 8630, 2695, 24748, 1012, 2069, 17015, 2532, 22091, 2319, 1999, 1996, 2599, 2535, 9020, 2000, 17894, 1999, 1037, 3459, 1997, 2061, 1011, 2170, 2329, 5848, 2008, 2057, 1005, 2222, 2763, 2196, 2963, 2013, 2153, 1012, 2012, 2560, 1010, 2008, 1005, 1055, 1996, 3246, 1012, 2279, 2051, 1010, 10887, 1037, 2367, 7464, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2178, 23824, 2245, 2003, 1996, 22293, 2135, 102]\n",
            "\n",
            "[CLS] token: [CLS], ID: 101\n",
            "\n",
            "[PAD] token: [PAD], ID: 0\n",
            "\n",
            "[SEP] token: [SEP], ID: 102\n",
            "\n",
            "Tokenized:  ['[CLS]', 'that', \"'\", 's', 'what', 'i', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fights', ',', 'screaming', 'matches', ',', 'swearing', 'and', 'general', 'mayhem', 'that', 'per', '##me', '##ate', 'the', '84', 'minutes', '.', 'the', 'comparisons', 'also', 'stand', 'up', 'when', 'you', 'think', 'of', 'the', 'one', '-', 'dimensional', 'characters', ',', 'who', 'have', 'so', 'little', 'depth', 'that', 'it', 'is', 'virtually', 'impossible', 'to', 'care', 'what', 'happens', 'to', 'them', '.', 'they', 'are', 'just', 'badly', 'written', 'cy', '##pher', '##s', 'for', 'the', 'director', 'to', 'hang', 'his', 'multicultural', 'beliefs', 'on', ',', 'a', 'topic', 'that', 'has', 'been', 'done', 'much', 'better', 'in', 'other', 'dramas', 'both', 'on', 'tv', 'and', 'the', 'cinema', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'must', 'confess', ',', 'i', \"'\", 'm', 'not', 'really', 'one', 'for', 'spotting', 'bad', 'performances', 'during', 'a', 'film', ',', 'but', 'it', 'must', 'be', 'said', 'that', 'nic', '##hol', '##a', 'bu', '##rley', '(', 'as', 'the', 'heroine', \"'\", 's', 'sl', '##ut', '##ty', 'best', 'friend', ')', 'and', 'was', '##im', 'za', '##kir', '(', 'as', 'the', 'nasty', ',', 'bullying', 'brother', ')', 'were', 'absolutely', 'terrible', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'acting', 'school', 'they', 'graduated', 'from', ',', 'but', 'if', 'i', 'was', 'them', 'i', \"'\", 'd', 'apply', 'for', 'a', 'full', 'ref', '##und', 'post', 'haste', '.', 'only', 'sami', '##na', 'aw', '##an', 'in', 'the', 'lead', 'role', 'manages', 'to', 'impress', 'in', 'a', 'cast', 'of', 'so', '-', 'called', 'british', 'talent', 'that', 'we', \"'\", 'll', 'probably', 'never', 'hear', 'from', 'again', '.', 'at', 'least', ',', 'that', \"'\", 's', 'the', 'hope', '.', 'next', 'time', ',', 'hire', 'a', 'different', 'scout', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'another', 'intriguing', 'thought', 'is', 'the', 'hideous', '##ly', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvi1_q33ybJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a46fde2-faf0-4988-ee56-8ca014727f6e"
      },
      "source": [
        "print(\"Max length: \", max([len(each) for each in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTelEtVNybJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec6e5ee-6f2c-4820-ccee-3c86950538c4"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
        "\n",
        "MAXLEN = 256\n",
        "\n",
        "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
        "                                                          maxlen=MAXLEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.4.1\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maKvU-KKybJZ"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTLBXYLTybJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8791ad37-3e3d-4244-d236-163961cc5d24"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, train_label, random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, valid_masks, _, _ = train_test_split(attention_masks, train_label, random_state=2018, test_size=0.1)\n",
        "\n",
        "print(train_inputs[:1])\n",
        "print(train_masks[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101  1045  1005  2310  3866  2023  3185  2144  1996  2034  2051  1045\n",
            "   2387  2009  8840  2122  2116  2086  3283  1012  1045  1005  1049  2025\n",
            "   2469  2129  2116  2335  1045  1005  2310  2464  2009  1010  3383  2184\n",
            "   1010  3383  2322  1012  2023  2197  2051  1045  3427  2009  1045  2001\n",
            "   4930  2011  1037  6987  2008  1045  2910  1005  1056  4384  2077  1012\n",
            "   1026  7987  1013  1028  1026  7987  1013  1028  2646  1996  2203  1997\n",
            "   1996  3861  1010  1996 19668  7348  2024 21527  2067  2000  1996  2237\n",
            "   3081 22889  2098  1012  2045  2024  1037  3232  1997  2485 22264  1997\n",
            "   1996  2757  2273  1012  1996  2028  2008  4930  2033  2087  2001  1037\n",
            "   2915  1997  1996  8855  3360  1012  2035  2017  2156  2006  1996  3898\n",
            "   2003  2010  6337  2013  2132  2000  2398  1012  2010  2398  2024  3173\n",
            "   1037 20046 13541  1012  1996  3612  2003 11221  1998  2010  4317  8855\n",
            "   2606  2003  5613  1999  1996  3612  1010  1999  8694  2007  1996 17909\n",
            "   1997  1996 13541  8457  1012  1996  5688  2090  2331  1998  1996  2166\n",
            "   2002  2038  2439  2003 11757  3928  1012  1996  3048  2606  1998 13541\n",
            "   8457 10825  2149  1997  1996  2166  2486  2008  2320  9613  2010  2303\n",
            "   1012  1026  7987  1013  1028  1026  7987  1013  1028  2296  2051  1045\n",
            "   7065 17417  2102  2023  2143  1045  2156  2242  2047  1012   102     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]]\n",
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hFzBxvyybJk"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "valid_inputs = torch.tensor(valid_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "valid_labels = torch.tensor(valid_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "valid_masks = torch.tensor(valid_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhbYgxdvybJq"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7JYZMxEybJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1b5492-6712-4f55-ed76-673a1e9d868e"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
        "                                                      num_labels = 2, \n",
        "                                                                      \n",
        "                                                      output_attentions = False, \n",
        "                                                      output_hidden_states = False) \n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN8StKalybJ5"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvTMmRTvybJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f127771a-f70f-48f0-d1c3-c223346d85c8"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "print(len(train_dataloader))\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pigy1CqrybKA"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=-1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXBpr-HAybKD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0QRDrYcybKG"
      },
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed_val):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOCgIIrpybKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c780f95-1010-4a8a-b687-2a1f9ba674ad"
      },
      "source": [
        "seed_val = 42\n",
        "set_seed(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        "   \n",
        "    t0 = time.time()\n",
        "    \n",
        "    total_loss = 0.\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}.\".format(step, len(train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              \n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    \n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    \n",
        "    t0 = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_acc = 0., 0.\n",
        "    \n",
        "    for valid_step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            \n",
        "        logits = outputs[0]\n",
        "        \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        tmp_eval_acc = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_acc += tmp_eval_acc        \n",
        "    \n",
        "    print(\"Accuracy: {0:.2f}\".format(eval_acc / (valid_step + 1)))\n",
        "    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "Batch    40 of 1,125. Elapsed: 0:00:56.\n",
            "Batch    80 of 1,125. Elapsed: 0:02:00.\n",
            "Batch   120 of 1,125. Elapsed: 0:03:05.\n",
            "Batch   160 of 1,125. Elapsed: 0:04:09.\n",
            "Batch   200 of 1,125. Elapsed: 0:05:14.\n",
            "Batch   240 of 1,125. Elapsed: 0:06:19.\n",
            "Batch   280 of 1,125. Elapsed: 0:07:23.\n",
            "Batch   320 of 1,125. Elapsed: 0:08:28.\n",
            "Batch   360 of 1,125. Elapsed: 0:09:32.\n",
            "Batch   400 of 1,125. Elapsed: 0:10:37.\n",
            "Batch   440 of 1,125. Elapsed: 0:11:42.\n",
            "Batch   480 of 1,125. Elapsed: 0:12:47.\n",
            "Batch   520 of 1,125. Elapsed: 0:13:52.\n",
            "Batch   560 of 1,125. Elapsed: 0:14:56.\n",
            "Batch   600 of 1,125. Elapsed: 0:16:01.\n",
            "Batch   640 of 1,125. Elapsed: 0:17:06.\n",
            "Batch   680 of 1,125. Elapsed: 0:18:11.\n",
            "Batch   720 of 1,125. Elapsed: 0:19:16.\n",
            "Batch   760 of 1,125. Elapsed: 0:20:21.\n",
            "Batch   800 of 1,125. Elapsed: 0:21:26.\n",
            "Batch   840 of 1,125. Elapsed: 0:22:31.\n",
            "Batch   880 of 1,125. Elapsed: 0:23:35.\n",
            "Batch   920 of 1,125. Elapsed: 0:24:40.\n",
            "Batch   960 of 1,125. Elapsed: 0:25:44.\n",
            "Batch 1,000 of 1,125. Elapsed: 0:26:49.\n",
            "Batch 1,040 of 1,125. Elapsed: 0:27:54.\n",
            "Batch 1,080 of 1,125. Elapsed: 0:28:59.\n",
            "Batch 1,120 of 1,125. Elapsed: 0:30:03.\n",
            "\n",
            "Average training loss: 0.26\n",
            "Training epoch took: 0:30:11\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.92\n",
            "Validation took: 0:01:12\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "Batch    40 of 1,125. Elapsed: 0:01:05.\n",
            "Batch    80 of 1,125. Elapsed: 0:02:09.\n",
            "Batch   120 of 1,125. Elapsed: 0:03:14.\n",
            "Batch   160 of 1,125. Elapsed: 0:04:19.\n",
            "Batch   200 of 1,125. Elapsed: 0:05:23.\n",
            "Batch   240 of 1,125. Elapsed: 0:06:28.\n",
            "Batch   280 of 1,125. Elapsed: 0:07:33.\n",
            "Batch   320 of 1,125. Elapsed: 0:08:38.\n",
            "Batch   360 of 1,125. Elapsed: 0:09:43.\n",
            "Batch   400 of 1,125. Elapsed: 0:10:47.\n",
            "Batch   440 of 1,125. Elapsed: 0:11:52.\n",
            "Batch   480 of 1,125. Elapsed: 0:12:56.\n",
            "Batch   520 of 1,125. Elapsed: 0:14:01.\n",
            "Batch   560 of 1,125. Elapsed: 0:15:05.\n",
            "Batch   600 of 1,125. Elapsed: 0:16:10.\n",
            "Batch   640 of 1,125. Elapsed: 0:17:15.\n",
            "Batch   680 of 1,125. Elapsed: 0:18:20.\n",
            "Batch   720 of 1,125. Elapsed: 0:19:24.\n",
            "Batch   760 of 1,125. Elapsed: 0:20:29.\n",
            "Batch   800 of 1,125. Elapsed: 0:21:34.\n",
            "Batch   840 of 1,125. Elapsed: 0:22:39.\n",
            "Batch   880 of 1,125. Elapsed: 0:23:44.\n",
            "Batch   920 of 1,125. Elapsed: 0:24:48.\n",
            "Batch   960 of 1,125. Elapsed: 0:25:53.\n",
            "Batch 1,000 of 1,125. Elapsed: 0:26:57.\n",
            "Batch 1,040 of 1,125. Elapsed: 0:28:02.\n",
            "Batch 1,080 of 1,125. Elapsed: 0:29:06.\n",
            "Batch 1,120 of 1,125. Elapsed: 0:30:11.\n",
            "\n",
            "Average training loss: 0.14\n",
            "Training epoch took: 0:30:19\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.93\n",
            "Validation took: 0:01:12\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "Batch    40 of 1,125. Elapsed: 0:01:05.\n",
            "Batch    80 of 1,125. Elapsed: 0:02:09.\n",
            "Batch   120 of 1,125. Elapsed: 0:03:14.\n",
            "Batch   160 of 1,125. Elapsed: 0:04:19.\n",
            "Batch   200 of 1,125. Elapsed: 0:05:23.\n",
            "Batch   240 of 1,125. Elapsed: 0:06:28.\n",
            "Batch   280 of 1,125. Elapsed: 0:07:33.\n",
            "Batch   320 of 1,125. Elapsed: 0:08:38.\n",
            "Batch   360 of 1,125. Elapsed: 0:09:43.\n",
            "Batch   400 of 1,125. Elapsed: 0:10:47.\n",
            "Batch   440 of 1,125. Elapsed: 0:11:52.\n",
            "Batch   480 of 1,125. Elapsed: 0:12:57.\n",
            "Batch   520 of 1,125. Elapsed: 0:14:02.\n",
            "Batch   560 of 1,125. Elapsed: 0:15:06.\n",
            "Batch   600 of 1,125. Elapsed: 0:16:09.\n",
            "Batch   640 of 1,125. Elapsed: 0:17:12.\n",
            "Batch   680 of 1,125. Elapsed: 0:18:15.\n",
            "Batch   720 of 1,125. Elapsed: 0:19:20.\n",
            "Batch   760 of 1,125. Elapsed: 0:20:24.\n",
            "Batch   800 of 1,125. Elapsed: 0:21:29.\n",
            "Batch   840 of 1,125. Elapsed: 0:22:34.\n",
            "Batch   880 of 1,125. Elapsed: 0:23:39.\n",
            "Batch   920 of 1,125. Elapsed: 0:24:44.\n",
            "Batch   960 of 1,125. Elapsed: 0:25:49.\n",
            "Batch 1,000 of 1,125. Elapsed: 0:26:53.\n",
            "Batch 1,040 of 1,125. Elapsed: 0:27:58.\n",
            "Batch 1,080 of 1,125. Elapsed: 0:29:03.\n",
            "Batch 1,120 of 1,125. Elapsed: 0:30:08.\n",
            "\n",
            "Average training loss: 0.08\n",
            "Training epoch took: 0:30:16\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.92\n",
            "Validation took: 0:01:12\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "Batch    40 of 1,125. Elapsed: 0:01:04.\n",
            "Batch    80 of 1,125. Elapsed: 0:02:08.\n",
            "Batch   120 of 1,125. Elapsed: 0:03:11.\n",
            "Batch   160 of 1,125. Elapsed: 0:04:15.\n",
            "Batch   200 of 1,125. Elapsed: 0:05:20.\n",
            "Batch   240 of 1,125. Elapsed: 0:06:24.\n",
            "Batch   280 of 1,125. Elapsed: 0:07:29.\n",
            "Batch   320 of 1,125. Elapsed: 0:08:34.\n",
            "Batch   360 of 1,125. Elapsed: 0:09:39.\n",
            "Batch   400 of 1,125. Elapsed: 0:10:43.\n",
            "Batch   440 of 1,125. Elapsed: 0:11:48.\n",
            "Batch   480 of 1,125. Elapsed: 0:12:53.\n",
            "Batch   520 of 1,125. Elapsed: 0:13:58.\n",
            "Batch   560 of 1,125. Elapsed: 0:15:02.\n",
            "Batch   600 of 1,125. Elapsed: 0:16:07.\n",
            "Batch   640 of 1,125. Elapsed: 0:17:12.\n",
            "Batch   680 of 1,125. Elapsed: 0:18:16.\n",
            "Batch   720 of 1,125. Elapsed: 0:19:21.\n",
            "Batch   760 of 1,125. Elapsed: 0:20:26.\n",
            "Batch   800 of 1,125. Elapsed: 0:21:30.\n",
            "Batch   840 of 1,125. Elapsed: 0:22:35.\n",
            "Batch   880 of 1,125. Elapsed: 0:23:39.\n",
            "Batch   920 of 1,125. Elapsed: 0:24:44.\n",
            "Batch   960 of 1,125. Elapsed: 0:25:49.\n",
            "Batch 1,000 of 1,125. Elapsed: 0:26:53.\n",
            "Batch 1,040 of 1,125. Elapsed: 0:27:58.\n",
            "Batch 1,080 of 1,125. Elapsed: 0:29:03.\n",
            "Batch 1,120 of 1,125. Elapsed: 0:30:07.\n",
            "\n",
            "Average training loss: 0.05\n",
            "Training epoch took: 0:30:15\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.92\n",
            "Validation took: 0:01:12\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HEWjeywybKL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "8fa6ad1a-1381-499a-d562-df90213a5405"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIg14zYC50F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9427eaa7-03f4-4da9-83f7-6e4c2322723f"
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for sent in test_data:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAXLEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(test_label)\n",
        " \n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PK69aZG6kWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcab39cb-a9c3-40b1-ae8d-fff9dccd3b9b"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "  true_labels.extend(label_ids.flatten())\n",
        "\n",
        "\n",
        "print('DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 10,000 test sentences...\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3o8SfUnV--K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bceddf-76d5-45ec-e0e6-b9d884539433"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "    \n",
        "target_names = ['negative', 'positive']\n",
        "\n",
        "print(classification_report(true_labels, predictions, digits=4, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.9398    0.8805    0.9091      4961\n",
            "    positive     0.8892    0.9444    0.9160      5039\n",
            "\n",
            "    accuracy                         0.9127     10000\n",
            "   macro avg     0.9145    0.9125    0.9126     10000\n",
            "weighted avg     0.9143    0.9127    0.9126     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}